<!DOCTYPE html>
<html>
<head>

	<title></title>
	<link rel="stylesheet" type="text/css" href="/pcd-delhi-workshop/style/main.css">
	<link rel="stylesheet" type="text/css" href="/pcd-delhi-workshop/style/highlighter.css">
	<script type="text/javascript" src="/pcd-delhi-workshop/scripts/prefixfree.min.js"></script>
</head>
<body>

	<link rel="stylesheet" type="text/css" href="/pcd-delhi-workshop/style/topbar.css">
<div id="topbar">
	<div id="links">
		<a href="/pcd-delhi-workshop/">home</a>
		<span>|</span>
		<a href="/pcd-delhi-workshop/syllabus">syllabus</a>
	</div>
	<div id="bar">
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
			<span></span>
		
	</div>
</div>

	<h1></h1>

	<ul>
  <li>Review of Notes</li>
  <li>Sound</li>
  <li>Analog Synth Components</li>
  <li>Synthesis</li>
  <li>Digital Audio</li>
  <li>DAWs</li>
  <li>Spectromorphology</li>
  <li>Dimensions and Parameters</li>
</ul>

<h2 id="basics-of-sound">BASICS OF SOUND</h2>

<p>Sound is a vibration that propagates as an audible mechanical wave of pressure and displacement, through a medium such as air or water.</p>

<h4 id="frequency">Frequency</h4>

<p>The number of occurrences of a repeating oscillation per unit time. 1 hertz is 1 cycle per second.</p>

<p>Frequency vs Pitch…</p>

<h4 id="wavelength">Wavelength</h4>

<p>The length of 1 cycle.</p>

<ul>
  <li>20hz wave is 17 meters long.</li>
  <li>20,000hz wave is 17 millimeters long.</li>
</ul>

<h4 id="amplitude">Amplitude</h4>

<p>The magnitude of the wave. The difference between extreme values. DIFFERENT than volume.</p>

<h4 id="phase">Phase</h4>

<p>Phase difference is the difference, expressed in degrees or time, between two waves having the same frequency and referenced to the same point in time.</p>

<h4 id="summing-and-subtracting-waves">Summing and Subtracting waves</h4>

<p>When two waves are in phase, their amplitudes sum. If they are out of phase, their amplitudes subtract.</p>

<h2 id="terminology">TERMINOLOGY</h2>

<ul>
  <li>Octave - the interval between one musical pitch and another with half or double its frequency.</li>
  <li>Decibel - a logarithmic scale used to measure sound level.</li>
  <li>Fundamental Frequency - the lowest frequency of a periodic waveform.</li>
  <li>Attenuation - decrease/increase in a signal. loudness.</li>
</ul>

<h2 id="analog-audio">ANALOG AUDIO</h2>

<p>When translating sound waves into electrical signals, synthesizers use voltage. A typical range would be ±15V or ±5V.</p>

<p>For demonstration, I will be using modular synths made with <a href="https://github.com/stretta/BEAP/wiki/BEAP-Modular---Overview-and-Install">BEAP</a> for Max/MSP.</p>

<h2 id="sources">Sources</h2>

<p>Outputs signal, but no input other than control voltage.</p>

<h3 id="oscillators">Oscillators</h3>

<p>VCO (Voltage-Controlled Oscillator). Oscillation frequency is controlled by a voltage input.</p>

<p>ARP 2600:</p>
<ul>
  <li>0v -&gt; 100hz</li>
  <li>+4v -&gt; 1600hz</li>
  <li>-4v -&gt; 6.25hz</li>
</ul>

<h4 id="sine">Sine</h4>

<p>In 1822, Joseph Fourier, a French mathematician, discovered that sinusoidal waves can be used as simple building blocks to describe and approximate any periodic waveform.</p>

<h4 id="square">Square</h4>

<p>Amplitude alternates at a steady frequency between fixed minimum and maximum values, with the same duration at minimum and maximum. The transition between minimum to maximum is instantaneous for an ideal square wave; this is not realizable in physical systems. Can be represented as an infinite summation of sinusoidal waves</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/b/bc/Fourier_series_for_square_wave.gif" alt="approximation with sine waves" /></p>

<h4 id="triangle">Triangle</h4>

<p>The triangle wave contains only odd harmonics, due to its odd symmetry. However, the higher harmonics roll off much faster than in a square wave.</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Synthesis_triangle.gif" alt="approximation with sine waves" /></p>

<h4 id="sawtooth">Sawtooth</h4>

<p>While a square wave is constructed from only odd harmonics, a sawtooth wave’s sound is harsh and clear and its spectrum contains both even and odd harmonics of the fundamental frequency. Because it contains all the integer harmonics, it is one of the best waveforms to use for subtractive synthesis of musical sounds, particularly bowed string instruments like violins and cellos.</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Synthesis_sawtooth.gif" alt="approximation with sine waves" /></p>

<h4 id="control-voltage">Control Voltage</h4>

<p>Signal which is used to control a parameter. i.e. Feeding an LFO into the frequency control of a VCO would create a frequency vibrato in which the frequency of the VCO warbles at the rate of the LFO.</p>

<h3 id="lfo">LFO</h3>

<p>Low Frequency Oscillator - slow cycles anywhere from a fortieth of a second to several minutes. Applied as control voltage since it’s not audible.</p>

<h3 id="vca">VCA</h3>

<p>Voltage Controlled Amplifier. Applying an LFO to VCA would make a tremolo.</p>

<h3 id="noise">Noise</h3>

<p><a href="https://en.wikipedia.org/wiki/Colors_of_noise">Multiple types of noise</a>: white, pink, brown, violet, blue…</p>

<h3 id="envelope-generator">Envelope Generator</h3>

<p>This is a control signal which rises and falls. Typically applied to the amplitude of a signal to create discrete “note” events.</p>

<h4 id="adsr">ADSR</h4>

<p>A very common synthesizer envelope which models musical instrument notes.</p>

<ul>
  <li>Attack time is the time taken for initial run-up of level from 0 to peak, beginning when the key is first pressed.</li>
  <li>Decay time is the time taken for the subsequent run down from the attack level to the designated sustain level.</li>
  <li>Sustain level is the level during the main sequence of the sound’s duration, until the key is released.</li>
  <li>Release time is the time taken for the level to decay from the sustain level to zero after the key is released.</li>
</ul>

<h2 id="processors">Processors</h2>

<h3 id="filters">Filters</h3>

<p>VCF - Voltage Controlled Filter. Attenuates frequencies below (high-pass), above (low-pass) or both below and above (band-pass) a certain frequency</p>

<h4 id="cutoff-frequency">Cutoff frequency</h4>

<p>At what frequency the filter starts to attenuate signal.</p>

<h4 id="rolloff">Rolloff</h4>

<p>How quickly the frequency attenuates. Measured as drop in db over octaves.</p>

<h4 id="types">Types</h4>

<ul>
  <li>Highpass - Only lets the high frequencies through.</li>
  <li>Lowpass - Only lets the low frequencies through.</li>
  <li>Bandpass - Only lets a narrow band of frequencies through.</li>
  <li>Notch - Rejects a narrow frequency band.</li>
  <li>High/Low shelf</li>
  <li>Peak</li>
</ul>

<h3 id="effects">Effects</h3>

<h4 id="delay">Delay</h4>

<p>Delay is an audio effect which records an input signal to an audio storage medium, and then plays it back after a period of time. When this signal is fed back into itself (with some attenuation usually) you get a feedback delay</p>

<h4 id="chorus">Chorus</h4>

<p>Chorus is an effect created with a delay which delays and detunes the input audio to thicken the sound.</p>

<h4 id="compressor">Compressor</h4>

<p>Compressors narrow the dynamic range (difference between the loud parts and the soft parts) of an audio signal. Typical controls are threshold (above which point the attenuation will occur) and ratio (the amount of attenuation to apply to signal above the threshold value).</p>

<h2 id="subtractive-and-additive-synthesis">Subtractive and Additive Synthesis</h2>

<p>Additive Synthesis combines sound generating elements (such as oscillators).</p>

<p>Subtractive Synthesis uses filters to subtract sound from a harmonically rich sound source.</p>

<h2 id="digital-audio">DIGITAL AUDIO</h2>

<h3 id="mixers-">Mixers (+)</h3>

<p>When two signals meet at a junction, the resulting signal is the sum of the inputs.</p>

<h3 id="gain-">Gain (*)</h3>

<p>Gain is the ratio between the input and the output signal.</p>

<h3 id="sampling">Sampling</h3>

<p>Continuous, analog values are converted to discrete digital events through sampling. The Nyquist Theorem states that the sampling rate needs to be at least twice the highest frequency to be reproduced.</p>

<p>Nyquist worked at Bell Labs and so did Max Matthews.</p>

<h3 id="aliasing">ALIASING</h3>

<p>Aliasing is an effect that causes different signals to become indistinguishable when sampled. When taking a video of a moving ceiling fan or car wheel you can get it to appear like it’s going backwards or much slower because the oscillation of the fan/wheel combined with the frame rate (sampling rate) of the camera.</p>

<h3 id="buffers">Buffers</h3>

<p>A buffer is an array. When audio is placed in a buffer and passed between sound producing/processing components.</p>

<h3 id="processing-graph">Processing Graph</h3>

<p>The chain of audio producing/processing components adds up to the processing graph.</p>

<h3 id="dac---digital-to-analog-converter">DAC - Digital-to-Analog Converter</h3>

<p>When audio is converted back into analog sound (like out your speaker), smoothing must be applied to get rid of the staircase effect between samples.</p>

<h3 id="noise-floor">Noise Floor</h3>

<p>The noise introduced by quantization error, including rounding errors and loss of precision introduced during audio processing, can be mitigated by adding a small amount of random noise, called dither, to the signal before quantizing. Dithering eliminates the granularity of quantization error, giving very low distortion, but at the expense of a slightly raised noise floor.</p>

<h2 id="daws">DAWs</h2>

<h3 id="channels">Channels</h3>

<p>Audio layers are arranged into channels with their own volume and pan controls. You can add effects to each channel.</p>

<h3 id="transport">Transport</h3>

<p>Play/Pause/Stop/Rewind/Loop</p>

<h3 id="automation">Automation</h3>

<p>Most DAWs let you automate nearly every parameter. A volume automation might be used to make an envelope or fade in/out.</p>

<h2 id="spectromorphology"><a href="https://github.com/igoumeninja/ofSpectrograph/blob/master/bibliography/Spectromorphology%20Explaining%20Sound%20Shapes_%20D.%20Smalley.pdf">Spectromorphology</a></h2>

<p>First published in 1986 by Denis Smally.</p>

<p>Addresses the inadequacy of traditional notation in capturing the endless sound-generating possibilities of computers.</p>

<p>If we can’t abstract them into instruments/notes/rhythms, how can we compose and reason about them?</p>

<blockquote>
  <p>Until the electroacoustic medium arrived, all music was created either through forms of vocal utterance or through instrumental gesture. A human agent produces spectromorphologies via the motion of gesture, using the sense of touch or an implement to apply energy to a sounding body. A gesture is therefore an energy–motion trajectory which excites the sounding body, creating spectromorphological life.</p>
</blockquote>

<h3 id="source-bonding">Source Bonding</h3>

<p>Associating sounds sources with what is perceived to have created that sound.</p>

<h4 id="ligeti---artikulation">Ligeti - <a href="https://www.youtube.com/watch?v=71hNl_skTZQ">Artikulation</a></h4>

<p>Score created by Rainer Wehinger after the piece was composed.</p>

<h3 id="gesture-and-its-surrogates">Gesture and Its Surrogates</h3>

<p>Before electroacoustic music, all sound was produced through sound-making gestures. From the perspective of the sound-creator and listener, the musical gesture process is tactile, visual and aural.</p>

<h3 id="onset-continuant-and-termination">Onset, Continuant and Termination</h3>

<ul>
  <li>attack-impulse. Modeled on the single detached note: a sudden onset which is immediately terminated.</li>
  <li>attack-decay (closed and open) - modeled on sounds in which the attack-onset is extended by a resonance that quickly or gradually decays towards termination. The closed form represents a quick decay which is strongly attack-determined.</li>
  <li>graduated continuant - Modeled on sustained sounds. The onset is graduated, settling into a continuant phase which eventually closes in a graduated termination.</li>
</ul>

<h2 id="dimensions">DIMENSIONS</h2>

<p>Dimensionality and parameterization are important concepts for interactive music. Often times users are given an interaction an a specific dimension. The higher-level the dimension (i.e. “energy” vs “loudness”, “dance-ability” vs “tempo”), the more engaging and approachable the interaction.</p>

<h3 id="in-class-exercise">IN CLASS EXERCISE</h3>

<p>Given a parameter curve, conceive a high-level dimension with your group and realize a short (30-60 second) piece of audio which follows that curve.</p>

<p>You can use any medium or software from live performance to GarageBand.</p>

<p>Groups of 3. 15 minutes presentation.</p>

<h2 id="references">REFERENCES</h2>

<ul>
  <li><a href="http://www.indiana.edu/~emusic/etext/digital_audio/chapter5_digital.shtml">Introduction to Computer Music: Digital Audio</a></li>
  <li><a href="http://beausievers.com/synth/synthbasics/">Intro to Synthesis</a></li>
  <li><a href="http://chimera.labs.oreilly.com/books/1234000001552/index.html">Web Audio - O’Reilly</a></li>
  <li><a href="http://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1417638918&amp;sr=1-1&amp;keywords=javascript+the+good+parts">Javascript: The Good Parts - Crockford, Chapters 2-4</a></li>
</ul>

<h2 id="reading">READING</h2>

<ul>
  <li><a href="https://github.com/igoumeninja/ofSpectrograph/blob/master/bibliography/Spectromorphology%20Explaining%20Sound%20Shapes_%20D.%20Smalley.pdf">Spectromorphology</a></li>
</ul>


	<link rel="stylesheet" type="text/css" href="/pcd-delhi-workshop/style/footer.css">
<div id='footnotes'>

</div>
<div id='footer'>
	©2017-2018 Yotam Mann <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a>
</div>

	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90915308-1', 'auto');
  ga('send', 'pageview');

</script>

	<script type="text/javascript">

	// jsfiddle creates too many AudioContexts, 
	// need to insure that the offscreen fiddles are closed
	// to remove the unused AudioContexts

	function isOffScreen (el) {
		var rect = el.getBoundingClientRect();
		return ((rect.left + rect.width) < 0 
			|| (rect.top + rect.height) < 0
			|| (rect.left > window.innerWidth || rect.top > window.innerHeight))
	}

	function reloadIframe(iframe){
		var url = iframe.src
		iframe.src = 'about:blank'
		setTimeout(function() {
			iframe.src = url
		}, 10)
	}

	//test the active elements to see if they are out of the viewport
	setInterval(function(){
		document.querySelectorAll('.active-iframe').forEach(function(el){
			if (isOffScreen(el)){
				reloadIframe(el)
				el.classList.remove('active-iframe')
			}
		})
	}, 1000)

	window.addEventListener('message', function(e){
		// if the results tab was selected
		var resultsTab = e.data[0] === 'resultsFrame'
		var slug = e.data[1].slug
		// get the iframe element using the slug
		document.querySelectorAll('iframe').forEach(function(iframe){
			if (iframe.src.indexOf(slug) !== -1){
				// mark the iframe as active if it's on the results tab
				if (resultsTab){
					iframe.classList.add('active-iframe')
				}
			}
		})
	})
</script>	
</body>
</html>